{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pHSAN0vzbOb"
   },
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tOr6nBPzYdY"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LWgW1QigytLs"
   },
   "outputs": [],
   "source": [
    "def count_label_occurances(label_list):\n",
    "  print('Counting occurrences of target classes:')\n",
    "  label_counts = pd.DataFrame(label_list, columns=['digit'])['digit'].value_counts()\n",
    "  label_percentages = (label_counts / label_counts.sum()) * 100\n",
    "\n",
    "  label_summary = pd.DataFrame({\n",
    "      'Count': label_counts,\n",
    "      'Percentage': label_percentages\n",
    "  }).sort_index()\n",
    "\n",
    "  print(label_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dwDOoxYOy0pA"
   },
   "outputs": [],
   "source": [
    "def generate_label_index_correspondance(images, labels):\n",
    "  images_and_labels = {}\n",
    "  for i in range (0, len(images)):\n",
    "    images_and_labels[i] = labels[i]\n",
    "  return images_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VP8HLJ5Ly1o2"
   },
   "outputs": [],
   "source": [
    "def print_shape(images, labels):\n",
    "  print(\"Shape of images: \", images.shape)\n",
    "  print(\"Shape of labels: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OE-1izm2zPDl"
   },
   "outputs": [],
   "source": [
    "def display_images_with_labels(images, labels, num_img=30, images_per_row=5, indices=None, random_subset=True):\n",
    "    if random_subset:\n",
    "        selected_indices = np.random.choice(range(len(images)), size=num_img, replace=False)\n",
    "    else:\n",
    "        if indices is None:\n",
    "            raise ValueError(\"Indices must be provided if random_subset is set to False.\")\n",
    "        selected_indices = indices\n",
    "\n",
    "    num_rows = (len(selected_indices) + images_per_row - 1) // images_per_row\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(images_per_row * 3, num_rows * 3))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        ax = axes[i]\n",
    "        img = images[idx]\n",
    "        img = np.squeeze(img)\n",
    "        ax.imshow(img, vmin=0., vmax=255.)\n",
    "        ax.set_title(f'Label: {labels[idx]}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YZ-R7KjqzRXw"
   },
   "outputs": [],
   "source": [
    "def remove_instances(images, labels, to_be_removed):\n",
    "\n",
    "    # Print initial shape\n",
    "    print(\"Shape before: \", images.shape, labels.shape)\n",
    "\n",
    "    # Remove duplicates\n",
    "    images_unique = np.delete(images, to_be_removed, axis=0)\n",
    "    labels_unique = np.delete(labels, to_be_removed, axis=0)\n",
    "\n",
    "    # Print resulting shape\n",
    "    print(\"Shape after: \", images_unique.shape, labels_unique.shape)\n",
    "\n",
    "    # Calculate and print the number of removed duplicates\n",
    "    num_removed = images.shape[0] - images_unique.shape[0]\n",
    "    print(f\"We removed {num_removed} images.\")\n",
    "\n",
    "    return images_unique, labels_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pnQFQCjczUc7"
   },
   "outputs": [],
   "source": [
    "def find_class_indices(labels):\n",
    "    class_indices = {}\n",
    "    for class_id in np.unique(labels):\n",
    "        class_indices[class_id] = np.where(labels == class_id)[0]\n",
    "    return class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "F0UiKyQHzVlB"
   },
   "outputs": [],
   "source": [
    "\n",
    "def select_random_subset(input_list, subset_size):\n",
    "    if subset_size > len(input_list):\n",
    "        raise ValueError(\"Subset size cannot be greater than the size of the input list.\")\n",
    "\n",
    "    indices = np.random.choice(len(input_list), subset_size, replace=False)\n",
    "    return [input_list[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2p3UTrP0zW1A"
   },
   "outputs": [],
   "source": [
    "def reshuffle(images, labels):\n",
    "    assert len(images) == len(labels), \"Images and labels arrays must have the same length.\"\n",
    "    rng = np.random.default_rng()\n",
    "    indices = rng.permutation(len(images))\n",
    "    images = images[indices]\n",
    "    labels = labels[indices]\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrDe9UROzhc4"
   },
   "source": [
    "#Initialization and data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "fZkS8WMXzjl4",
    "outputId": "e8ef15a3-bfdf-4b10-92ed-4531d4ccb57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-cv\n",
      "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (24.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2024.9.11)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.7)\n",
      "Collecting keras-core (from keras-cv)\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-cv) (0.3.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-cv) (4.66.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (3.12.1)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.25.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.13.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.5.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.16.0)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.5.1)\n",
      "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (6.4.5)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-cv) (2024.8.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.18.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras-cv) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.66.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
      "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
      "Successfully installed keras-core-0.1.7 keras-cv-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install for access to RandAugment\n",
    "\n",
    "!pip install keras-cv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIn5ii1czqHH",
    "outputId": "884373ed-f8ed-41b4-b9c0-2041a2cf497a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.1\n"
     ]
    }
   ],
   "source": [
    "# Miscellaneous imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "\n",
    "# SciKit imports\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from keras_cv.layers import RandAugment\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "\n",
    "\n",
    "# Model-specific imports\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobilenet\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Define random seed for reprodusability\n",
    "seed = 69\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hX_GFBUe045b"
   },
   "outputs": [],
   "source": [
    "# Import dataset and define integer-name correspondance for convenience when displaying images\n",
    "\n",
    "data = np.load('training_set.npz')\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "\n",
    "labels = {\n",
    "0: 'Basophil',\n",
    "1: 'Eosinophil',\n",
    "2: 'Erythroblast',\n",
    "3: 'Immature granulocytes',\n",
    "4: 'Lymphocyte',\n",
    "5: 'Monocyte',\n",
    "6: 'Neutrophil',\n",
    "7: 'Platelet',\n",
    "}\n",
    "unique_labels = list(labels.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p43JpfkF1KL9"
   },
   "source": [
    "# Removing duplicate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCuvy2HY1L73",
    "outputId": "ea67372d-6e3e-4ad4-9b60-761aa7ff4bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806  duplicates located.\n"
     ]
    }
   ],
   "source": [
    "# Locate the duplicate images\n",
    "\n",
    "def find_duplicate_images(images, labels):\n",
    "    hashes = {}\n",
    "    duplicates = {}\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img_flat = img.tobytes()\n",
    "        img_hash = hashlib.md5(img_flat).hexdigest()\n",
    "\n",
    "        if img_hash in hashes:\n",
    "            first_index = hashes[img_hash]\n",
    "            duplicates[i] = {'label': labels[i], 'first_index': first_index}\n",
    "        else:\n",
    "            hashes[img_hash] = i  #\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "duplicates = find_duplicate_images(X, y)\n",
    "to_be_removed = list(duplicates.keys())\n",
    "print(len(duplicates.keys()), \" duplicates located.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UwQJoUM1dwg",
    "outputId": "b79d5a52-0c05-4f15-9581-1bcc2f2dda2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before:  (13759, 96, 96, 3) (13759, 1)\n",
      "Shape after:  (11951, 96, 96, 3) (11951, 1)\n",
      "We removed 1808 images.\n"
     ]
    }
   ],
   "source": [
    "# to_be_removed contains the indices at which you will find the duplicates.\n",
    "# For the legitimate images no more needs to be done, but as we do not want ANY copies\n",
    "# of the illegitimate images (Rick Astley and astonaut(?)), we will manually append\n",
    "# the indices at which these first occur\n",
    "\n",
    "to_be_removed.append(13559)\n",
    "to_be_removed.append(11959)\n",
    "\n",
    "X_unique, y_unique = remove_instances(X, y, to_be_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLhZAxVK2P7j",
    "outputId": "72d77bc9-f40b-4d48-b88c-8ca01779cc75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3440 (label: 1) is removed:\n",
      "Index: 3437 label:  [5]\n",
      "Index: 3438 label:  [1]\n",
      "Index: 3439 label:  [5]\n",
      "Index: 3440 label:  [1]\n",
      "Index: 3441 label:  [5]\n",
      "Index: 3442 label:  [7]\n",
      "\n",
      "\n",
      "Index: 3437 label:  [5]\n",
      "Index: 3438 label:  [1]\n",
      "Index: 3439 label:  [5]\n",
      "Index: 3440 label:  [5]\n",
      "Index: 3441 label:  [7]\n",
      "Index: 3442 label:  [5]\n",
      "\n",
      "\n",
      "3440 (label: 1) and 4761 (label: 3) is removed:\n",
      "Index: 4758 label:  [4]\n",
      "Index: 4759 label:  [3]\n",
      "Index: 4760 label:  [0]\n",
      "Index: 4761 label:  [3]\n",
      "Index: 4762 label:  [6]\n",
      "Index: 4763 label:  [2]\n",
      "\n",
      "\n",
      "Index: 4758 label:  [3]\n",
      "Index: 4759 label:  [0]\n",
      "Index: 4760 label:  [6]\n",
      "Index: 4761 label:  [2]\n",
      "Index: 4762 label:  [5]\n",
      "Index: 4763 label:  [5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NO IMPORTANT FUNCTIONALITY\n",
    "# Just a check that the image-label correspondance is still correct (it is)\n",
    "\n",
    "label_index_correspondance = generate_label_index_correspondance(X, y)\n",
    "unique_label_index_correspondance = generate_label_index_correspondance(X_unique, y_unique)\n",
    "\n",
    "\n",
    "print(\"3440 (label: 1) is removed:\")\n",
    "for i in range(3437,3443):\n",
    "  print(\"Index:\", i, \"label: \", label_index_correspondance[i])\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in range(3437,3443):\n",
    "  print(\"Index:\", i, \"label: \", unique_label_index_correspondance[i])\n",
    "#print(label_index_correspondance[3437:3443])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3440 (label: 1) and 4761 (label: 3) is removed:\")\n",
    "for i in range(4758,4764):\n",
    "  print(\"Index:\", i, \"label: \", label_index_correspondance[i])\n",
    "\n",
    "print(\"\\n\")\n",
    "for i in range(4758,4764):\n",
    "  print(\"Index:\", i, \"label: \", unique_label_index_correspondance[i])\n",
    "\n",
    "# Garbage collection\n",
    "del label_index_correspondance\n",
    "del unique_label_index_correspondance\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Zs7vD6M3c2R"
   },
   "source": [
    "# Splitting into validation and training sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHlrhUJ33dQi",
    "outputId": "87ef2763-1a9f-41ac-9a28-d7f83ac6eca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting occurrences of target classes:\n",
      "       Count  Percentage\n",
      "digit                   \n",
      "0        850    7.112376\n",
      "1       2179   18.232784\n",
      "2       1085    9.078738\n",
      "3       2023   16.927454\n",
      "4        849    7.104008\n",
      "5        992    8.300561\n",
      "6       2330   19.496276\n",
      "7       1643   13.747804\n"
     ]
    }
   ],
   "source": [
    "# Printing the label distribution for the dataset where duplicates have been removed\n",
    "# (for later reference)\n",
    "\n",
    "count_label_occurances(y_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq0u_YT43gdx",
    "outputId": "207b536f-1652-4eea-8e41-5438c7b9a573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class sample indices (first 5 per class):\n",
      "Class 0: [26 38 42 47 79]\n",
      "Class 1: [10 11 19 21 32]\n",
      "Class 2: [ 6  7  9 17 31]\n",
      "Class 3: [ 1 15 30 41 46]\n",
      "Class 4: [28 37 39 52 53]\n",
      "Class 5: [22 23 29 45 49]\n",
      "Class 6: [ 2  3  5 12 14]\n",
      "Class 7: [ 0  4  8 13 16]\n"
     ]
    }
   ],
   "source": [
    "# Creates a map class_indices where the keys are the labels as integers,\n",
    "# and the values are NumPy arrays containing the indices corresponding to\n",
    "# the images that belong to that label.\n",
    "\n",
    "class_indices = find_class_indices(y_unique)\n",
    "\n",
    "# To illustrate structure:\n",
    "class_sample_indices = {cls: indices[:5] for cls, indices in class_indices.items()}\n",
    "print(\"Class sample indices (first 5 per class):\")\n",
    "for cls, indices in class_sample_indices.items():\n",
    "    print(f\"Class {cls}: {indices[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4jRFe_R31L8",
    "outputId": "d8063bd2-d4eb-4ac4-ed04-011200d8de3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "# Set aside 80 % of data set for use as a validation set\n",
    "train_end_idx = int(0.8 * len(X_unique))\n",
    "\n",
    "# Calculate X where X is the amount of individual images from each label\n",
    "# to be used in the validation set - we want to ensure equal representation\n",
    "# from each label in the validation set\n",
    "instances_from_each_label_for_val = (len(X_unique) - train_end_idx)//8\n",
    "print(instances_from_each_label_for_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rg_lrgL88tpI",
    "outputId": "d4510f27-f0d7-4498-99d0-d4ec50a72757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the validation set: \n",
      "\n",
      "Shape of images:  (2384, 96, 96, 3)\n",
      "Shape of labels:  (2384, 1)\n",
      "Shape of the training set, before and after: \n",
      "\n",
      "Shape before:  (11951, 96, 96, 3) (11951, 1)\n",
      "Shape after:  (9567, 96, 96, 3) (9567, 1)\n",
      "We removed 2384 images.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each label, we randomly choose instances_from_each_label_for_val images to be included in the validation set\n",
    "# The chosen image-label pairs are subsequently removed from the training set\n",
    "\n",
    "validation_images = []\n",
    "validation_lables = []\n",
    "relevant_indices_and_labels = {}\n",
    "\n",
    "for relevant_label in range(0, len(unique_labels)):\n",
    "    relevant_indices = class_indices[relevant_label]\n",
    "\n",
    "    random_indices = select_random_subset(relevant_indices, instances_from_each_label_for_val)\n",
    "\n",
    "    for i in random_indices:\n",
    "        validation_images.append(X_unique[i])\n",
    "        validation_lables.append([relevant_label])\n",
    "        relevant_indices_and_labels[i] = relevant_label\n",
    "\n",
    "\n",
    "to_be_removed = list(relevant_indices_and_labels.keys())\n",
    "\n",
    "X_val_raw = np.array(validation_images)\n",
    "y_val_raw = np.array(validation_lables)\n",
    "X_val_raw, y_val_raw = reshuffle(X_val_raw, y_val_raw)\n",
    "\n",
    "print(\"Shape of the validation set: \\n\")\n",
    "print_shape(X_val_raw, y_val_raw)\n",
    "\n",
    "print(\"Shape of the training set, before and after: \\n\")\n",
    "X_train_raw, y_train_raw = remove_instances(X_unique, y_unique, to_be_removed)\n",
    "\n",
    "# Garbage collection\n",
    "del X_unique\n",
    "del y_unique\n",
    "del to_be_removed\n",
    "del class_indices\n",
    "del class_sample_indices\n",
    "del train_end_idx\n",
    "del instances_from_each_label_for_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMW7Xyya9RkT",
    "outputId": "0b29e8b2-b1d1-4ace-fa7b-aa978e3bc01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting occurrences of target classes:\n",
      "       Count  Percentage\n",
      "digit                   \n",
      "0        298        12.5\n",
      "1        298        12.5\n",
      "2        298        12.5\n",
      "3        298        12.5\n",
      "4        298        12.5\n",
      "5        298        12.5\n",
      "6        298        12.5\n",
      "7        298        12.5\n"
     ]
    }
   ],
   "source": [
    "# Ensure logic was sound, and that we now have an equal label representation in validation set\n",
    "\n",
    "count_label_occurances(y_val_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fcoxY4M90zc"
   },
   "source": [
    "# Augmenting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xzHDq81T9h5V"
   },
   "outputs": [],
   "source": [
    "# Update class_indices to correspond to the new training set (after split)\n",
    "class_indices = find_class_indices(y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dheqZ0X_-CI8"
   },
   "outputs": [],
   "source": [
    "# Augment data\n",
    "def augment_data(X, Y, augmenter):\n",
    "    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    augmented_images = augmenter(X_tensor)\n",
    "    augmented_images_np = augmented_images.numpy()\n",
    "\n",
    "    # We augment the images \"in place\" and add no new ones, so Y is just passed through\n",
    "    return augmented_images_np, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gTkJD9cT_jyt"
   },
   "outputs": [],
   "source": [
    "# Define the augmenter to be passed to the augment_data function, dictating the intensity and\n",
    "# amount per image of augmentations.\n",
    "\n",
    "# Here, we defined three main levels for ourselves:\n",
    "# 1. Baseline augmentation: augmentations_per_image=1, magnitude=0.2\n",
    "  # -> Used in the main dataset for finding our baseline model (see report)\n",
    "\n",
    "# 2. Low augmentation: augmentations_per_image=1, magnitude=0.35\n",
    "# 3. Medium augmentation: augmentations_per_image=2, magnitude=0.45\n",
    "# 4. High augmentation: augmentations_per_image=2, magnitude=0.6\n",
    "\n",
    "# We experimented with all of these levels, see report for result.\n",
    "\n",
    "rand_augment = RandAugment(\n",
    "    value_range=(0, 255),\n",
    "    augmentations_per_image=2,\n",
    "    magnitude=0.45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "a3FqeZU5BNVP"
   },
   "outputs": [],
   "source": [
    "# Augment the training set\n",
    "X_train_augmented, y_train_augmented = augment_data(X_train_raw, y_train_raw, rand_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pQC9orDFBeqp"
   },
   "outputs": [],
   "source": [
    "# Augment the validation set, to hopefully closer resemble the hidden test set\n",
    "X_val_augmented, y_val_augmented = augment_data(X_val_raw, y_val_raw, rand_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miLSTYWCBpnf",
    "outputId": "121a286d-6f9e-4de7-ae2f-4ee08636fe28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: \n",
      "Shape of images:  (9567, 96, 96, 3)\n",
      "Shape of labels:  (9567, 1)\n",
      "Validation set shape: \n",
      "Shape of images:  (2384, 96, 96, 3)\n",
      "Shape of labels:  (2384, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8859"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure no funny business\n",
    "print(\"Training set shape: \")\n",
    "print_shape(X_train_augmented, y_train_augmented)\n",
    "print(\"Validation set shape: \")\n",
    "print_shape(X_val_augmented, y_val_augmented)\n",
    "\n",
    "# Garbage collection\n",
    "del X_train_raw\n",
    "del y_train_raw\n",
    "del rand_augment\n",
    "del X_val_raw\n",
    "del y_val_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjdOwzBxDRtg"
   },
   "source": [
    "# Further augmentation: increasing data set size by rotating and flipping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lNZDkuyJB6dS"
   },
   "outputs": [],
   "source": [
    "def rotate_image_90(image, amount_of_rotations=1):\n",
    "    return np.rot90(image, k=amount_of_rotations, axes=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sg1BEId0DxJl",
    "outputId": "a5da8410-bee5-4fc4-8c85-95b64ca64b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images:  (38268, 96, 96, 3)\n",
      "Shape of labels:  (38268, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rotate each of the augmented images three times, so that \"each image becomes four\"\n",
    "\n",
    "# Input images and labels\n",
    "images = X_train_augmented\n",
    "labels = y_train_augmented\n",
    "\n",
    "class_indices = find_class_indices(labels)\n",
    "\n",
    "images_list = list(images)\n",
    "labels_list = list(labels)\n",
    "\n",
    "for relevant_label in range(0, len(unique_labels)):\n",
    "    relevant_indices = class_indices[relevant_label]\n",
    "    relevant_images = []\n",
    "\n",
    "    # Collect all images for the current label\n",
    "    for i in relevant_indices:\n",
    "        relevant_images.append(images[i])\n",
    "\n",
    "    rotations = 3 # We want images at 0, 90, 180, 270 degrees\n",
    "    for image in relevant_images:\n",
    "      for _ in range(rotations):\n",
    "          rotated_image = rotate_image_90(image)\n",
    "\n",
    "          # To avoid \"layers\" of same label\n",
    "          insert_position = np.random.randint(0, len(images_list) + 1)\n",
    "          images_list.insert(insert_position, rotated_image)\n",
    "          labels_list.insert(insert_position, [relevant_label])\n",
    "\n",
    "\n",
    "# Convert back to NumPy arrays\n",
    "X_train_rotated = np.array(images_list)\n",
    "y_train_rotated = np.array(labels_list)\n",
    "\n",
    "# Ensure the dataset quadrupled in size\n",
    "print_shape(X_train_rotated, y_train_rotated)\n",
    "\n",
    "# Garbage collection\n",
    "del images\n",
    "del labels\n",
    "del X_train_augmented\n",
    "del y_train_augmented\n",
    "del class_indices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ng6I5GIDFJ05",
    "outputId": "1d4bd50a-3e57-47ec-a3fc-7d47bf25c586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images:  (76536, 96, 96, 3)\n",
      "Shape of labels:  (76536, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To each of the rotated images, apply a flip about the y_axis.\n",
    "# Append the new image to the dataset, along with its corresponding label\n",
    "\n",
    "def augment_with_flipping(images, labels):\n",
    "    images_list = list(images)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        # Flip horizontally\n",
    "        flipped_h = np.flip(image, axis=1)\n",
    "        images_list.append(flipped_h)\n",
    "        labels_list.append(label)\n",
    "    return np.array(images_list), np.array(labels_list)\n",
    "\n",
    "X_train_flipped, y_train_flipped = augment_with_flipping(X_train_rotated, y_train_rotated)\n",
    "\n",
    "# Shuffle dataset\n",
    "reshuffle(X_train_flipped, y_train_flipped)\n",
    "\n",
    "# Ensure the dataset doubled in size\n",
    "print_shape(X_train_flipped, y_train_flipped)\n",
    "\n",
    "# Garbage collection\n",
    "del X_train_rotated\n",
    "del y_train_rotated\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oTI1ZAEHYc2",
    "outputId": "460cdb36-0136-4301-bec5-b1c4829f317d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: \n",
      "Shape of images:  (76536, 96, 96, 3)\n",
      "Shape of labels:  (76536, 1)\n",
      "\n",
      "\n",
      "Validation set shape: \n",
      "Shape of images:  (2384, 96, 96, 3)\n",
      "Shape of labels:  (2384, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename for convenience and clarity in later stages\n",
    "X_train = X_train_flipped\n",
    "y_train = y_train_flipped\n",
    "X_val = X_val_augmented\n",
    "y_val = y_val_augmented\n",
    "\n",
    "print(\"Training set shape: \")\n",
    "print_shape(X_train, y_train)\n",
    "print(\"\\n\")\n",
    "print(\"Validation set shape: \")\n",
    "print_shape(X_val, y_val)\n",
    "\n",
    "# Garbage collection\n",
    "del X_train_flipped\n",
    "del y_train_flipped\n",
    "del X_val_augmented\n",
    "del y_val_augmented\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR4Bj7ysOlZm"
   },
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUahzHeXOrdk"
   },
   "source": [
    "## ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "l1S9obDOOLwd"
   },
   "outputs": [],
   "source": [
    "def preprocess_images_in_batches_resnet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_resnet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch (not strictly necessary in Python)\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ONEHBuB5PdNE"
   },
   "outputs": [],
   "source": [
    "# Scale input to ResNet-specific format\n",
    "X_train_RN = preprocess_images_in_batches_resnet(X_train)\n",
    "X_val_RN = preprocess_images_in_batches_resnet(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EpVSjRbcPf7R"
   },
   "outputs": [],
   "source": [
    "# Make label tensors one-hot encoded\n",
    "y_train_RN = to_categorical(y_train)\n",
    "y_val_RN = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfp2h-vWP0AK",
    "outputId": "cceeb3dd-0abb-4c90-d438-2f1470bd3583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (96, 96, 3)\n",
      "Output Shape: 8\n"
     ]
    }
   ],
   "source": [
    "# Input shape for the model\n",
    "input_shape_RN = X_train_RN.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape_RN = y_train_RN.shape[1]\n",
    "\n",
    "print(\"Input Shape:\", input_shape_RN)\n",
    "print(\"Output Shape:\", output_shape_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jorrQwV_P5fv",
    "outputId": "5a691dee-27f1-4c3b-b227-9b47ed712f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1000\n",
      "Batch Size: 32\n",
      "Learning Rate: 8e-05\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "\n",
    "# Number of training epochs\n",
    "epochs_RN = 1000\n",
    "\n",
    "# Batch size for training\n",
    "batch_size_RN = 32\n",
    "\n",
    "# Learning rate: step size for updating the model's weights\n",
    "learning_rate_RN = 0.00008\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs_RN)\n",
    "print(\"Batch Size:\", batch_size_RN)\n",
    "print(\"Learning Rate:\", learning_rate_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qm1TRaNHQBcz",
    "outputId": "158f92da-2e2f-4f9c-8a11-de5f74b86805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 2.166440217391304, 1: 0.6357655502392344, 2: 1.519536213468869, 3: 0.6932608695652174, 4: 2.170372050816697, 5: 1.7231628242074928, 6: 0.5885211614173228, 7: 0.8891263940520446}\n"
     ]
    }
   ],
   "source": [
    "# Calculate appropriate class weights to make up for dataset imbalance\n",
    "\n",
    "classes = np.unique(np.argmax(y_train_RN, axis=1))  # Get unique class labels from one-hot encoded y_train\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=np.argmax(y_train_RN, axis=1))\n",
    "class_weight_dict_RN = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "6whCcgW_SMuK"
   },
   "outputs": [],
   "source": [
    "# Define the patience value for early stopping\n",
    "patience_RN = 5\n",
    "\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping_RN = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=patience_RN,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks_RN = [early_stopping_RN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj3alesE0iq7"
   },
   "source": [
    "### With no fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "UNu2b_bmfqVU"
   },
   "outputs": [],
   "source": [
    "# Non fine-tunable implementation of ResNet50\n",
    "# Not our flagship model, but the one used to decide on a baseline-model ref. report\n",
    "\n",
    "def build_resnet(\n",
    "    input_shape=input_shape_RN,\n",
    "    output_shape=output_shape_RN,\n",
    "    learning_rate=learning_rate_RN,\n",
    "    seed=seed\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='Input')\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=seed, name='dropout_1')(x)\n",
    "    outputs = tf.keras.layers.Dense(units=output_shape, activation='softmax', dtype='float32', name='output')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='ResNet50')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    mixed_precision_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=mixed_precision_optimizer, metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "u2-41__gSJYG",
    "outputId": "966fed6f-6b7b-445c-f33c-4f3cf2ebd6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ResNet50\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ResNet50\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m262,272\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cast_1 (\u001b[38;5;33mCast\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,016</span> (90.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,016\u001b[0m (90.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">263,304</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m263,304\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NB! Change call to construct the non fine-tunable ResNet-model if you wish\n",
    "# -> And don't forget to adjust the learning rate accordingly!\n",
    "\n",
    "RN = build_resnet()\n",
    "#RN = build_fine_tuned_resnet()\n",
    "RN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVp_pJGDSXRK",
    "outputId": "bda98293-2160-4ac1-d6de-85810218449d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 16ms/step - accuracy: 0.5661 - loss: 1.2975 - precision: 0.6874 - recall: 0.4347 - val_accuracy: 0.8108 - val_loss: 0.5420 - val_precision: 0.8636 - val_recall: 0.7567\n",
      "Epoch 2/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.8270 - loss: 0.4794 - precision: 0.8754 - recall: 0.7730 - val_accuracy: 0.8523 - val_loss: 0.4461 - val_precision: 0.8907 - val_recall: 0.8201\n",
      "Epoch 3/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.8678 - loss: 0.3555 - precision: 0.9023 - recall: 0.8332 - val_accuracy: 0.8683 - val_loss: 0.4117 - val_precision: 0.8926 - val_recall: 0.8402\n",
      "Epoch 4/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.8954 - loss: 0.2842 - precision: 0.9206 - recall: 0.8692 - val_accuracy: 0.8800 - val_loss: 0.3903 - val_precision: 0.8982 - val_recall: 0.8582\n",
      "Epoch 5/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9132 - loss: 0.2340 - precision: 0.9320 - recall: 0.8930 - val_accuracy: 0.8788 - val_loss: 0.3792 - val_precision: 0.8983 - val_recall: 0.8599\n",
      "Epoch 6/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9256 - loss: 0.1998 - precision: 0.9407 - recall: 0.9088 - val_accuracy: 0.8763 - val_loss: 0.3806 - val_precision: 0.8953 - val_recall: 0.8641\n",
      "Epoch 7/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9345 - loss: 0.1748 - precision: 0.9477 - recall: 0.9221 - val_accuracy: 0.8788 - val_loss: 0.3777 - val_precision: 0.8986 - val_recall: 0.8700\n",
      "Epoch 8/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9434 - loss: 0.1501 - precision: 0.9533 - recall: 0.9316 - val_accuracy: 0.8817 - val_loss: 0.3738 - val_precision: 0.8977 - val_recall: 0.8691\n",
      "Epoch 9/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9483 - loss: 0.1345 - precision: 0.9575 - recall: 0.9384 - val_accuracy: 0.8784 - val_loss: 0.3885 - val_precision: 0.8963 - val_recall: 0.8700\n",
      "Epoch 10/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9552 - loss: 0.1181 - precision: 0.9629 - recall: 0.9482 - val_accuracy: 0.8830 - val_loss: 0.3817 - val_precision: 0.8978 - val_recall: 0.8767\n",
      "Epoch 11/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9588 - loss: 0.1073 - precision: 0.9654 - recall: 0.9521 - val_accuracy: 0.8796 - val_loss: 0.3903 - val_precision: 0.8929 - val_recall: 0.8708\n",
      "Epoch 12/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9638 - loss: 0.0950 - precision: 0.9693 - recall: 0.9588 - val_accuracy: 0.8729 - val_loss: 0.4185 - val_precision: 0.8864 - val_recall: 0.8674\n",
      "Epoch 13/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9664 - loss: 0.0881 - precision: 0.9713 - recall: 0.9619 - val_accuracy: 0.8758 - val_loss: 0.4128 - val_precision: 0.8858 - val_recall: 0.8683\n",
      "Epoch 14/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9703 - loss: 0.0807 - precision: 0.9740 - recall: 0.9659 - val_accuracy: 0.8754 - val_loss: 0.4162 - val_precision: 0.8876 - val_recall: 0.8679\n",
      "Epoch 15/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.9737 - loss: 0.0733 - precision: 0.9767 - recall: 0.9692 - val_accuracy: 0.8779 - val_loss: 0.4260 - val_precision: 0.8867 - val_recall: 0.8729\n",
      "Final validation accuracy: 88.3%\n",
      "Saved model as  ResNetFineTuned_forShow88.3.keras\n"
     ]
    }
   ],
   "source": [
    "# ResNet training (NO FINE TUNING - ignore the file name)\n",
    "# Trained on the baseline level of augmentation, mimicking the test we did to choose our baseline model\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = RN.fit(\n",
    "    x=X_train_RN,\n",
    "    y=y_train_RN,\n",
    "    batch_size=batch_size_RN,\n",
    "    epochs=epochs_RN,\n",
    "    validation_data=(X_val_RN, y_val_RN),\n",
    "    callbacks=callbacks_RN,\n",
    "    class_weight=class_weight_dict_RN\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "model_filename = 'ResNetTrulyFineTuned_forShow'+str(final_val_accuracy)+'.keras'\n",
    "RN.save(model_filename)\n",
    "print(\"Saved model as \", model_filename)\n",
    "\n",
    "# Delete the model to free up resources\n",
    "del RN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVrD_Eia063E"
   },
   "source": [
    "### With fine tuning (our best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "itfovk0xQr0u"
   },
   "outputs": [],
   "source": [
    "# Fine-tunable implementation of ResNet50\n",
    "\n",
    "def build_fine_tuned_resnet(\n",
    "    input_shape=input_shape_RN,\n",
    "    output_shape=output_shape_RN,\n",
    "    base_learning_rate=learning_rate_RN,\n",
    "    seed=seed\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Enable mixed precision\n",
    "    set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision policy set to: 'mixed_float16'\")\n",
    "\n",
    "    # Load ResNet-50 as the base model\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Defining classification head - GAP, one dense layer, ReLu and dropout prob. = 0.3\n",
    "    inputs = tfkl.Input(shape=input_shape, name='Input')\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = tfkl.Dense(128, activation='relu', name='dense_1')(x)  # Directly apply activation here\n",
    "    x = tfkl.Dropout(0.3, seed=seed, name='dropout_1')(x)\n",
    "    outputs = tfkl.Dense(units=output_shape, activation='softmax', dtype='float32', name='output')(x)\n",
    "\n",
    "\n",
    "    # When we experimented with Batch Normalization, the classification head in stead looked like this:\n",
    "\n",
    "    #inputs = tf.keras.layers.Input(shape=input_shape, name='Input')\n",
    "    #x = base_model(inputs, training=False)\n",
    "    #x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    #x = tf.keras.layers.Dense(128, activation=None, name='dense_1')(x)  # Remove activation here\n",
    "    #x = tf.keras.layers.BatchNormalization(name='batch_norm_1')(x)       # Add BatchNorm\n",
    "    #x = tf.keras.layers.Activation('relu', name='relu_activation_1')(x) # Add activation after BatchNorm\n",
    "    #x = tf.keras.layers.Dropout(0.3, seed=seed, name='dropout_1')(x)\n",
    "    #outputs = tf.keras.layers.Dense(units=output_shape, activation='softmax', name='output')(x)\n",
    "    # <- NB! With BatchNorm, remember to increase the batch size!\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='ResNet50_FineTuned')\n",
    "\n",
    "    # For mixed precision compatibility\n",
    "    adam_optimizer = Adam(learning_rate=base_learning_rate)\n",
    "    mixed_precision_optimizer = LossScaleOptimizer(adam_optimizer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=mixed_precision_optimizer, metrics=['accuracy', 'precision', 'recall'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "Zgq_iZjaxYTH",
    "outputId": "5ea7abca-ac69-4e5c-a9b7-69303be610bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision policy set to: 'mixed_float16'\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ResNet50_FineTuned\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ResNet50_FineTuned\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m262,272\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ cast_1 (\u001b[38;5;33mCast\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,851,016</span> (90.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,851,016\u001b[0m (90.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,797,896</span> (90.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,797,896\u001b[0m (90.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#RN = build_resnet()\n",
    "RN = build_fine_tuned_resnet()\n",
    "RN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqL0OIwJxqR2",
    "outputId": "455bba17-5b6a-4a55-eeb4-2baabd734290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 56ms/step - accuracy: 0.8216 - loss: 0.5414 - precision: 0.8911 - recall: 0.7732 - val_accuracy: 0.9228 - val_loss: 0.2744 - val_precision: 0.9288 - val_recall: 0.9190\n",
      "Epoch 2/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 38ms/step - accuracy: 0.9748 - loss: 0.0744 - precision: 0.9781 - recall: 0.9714 - val_accuracy: 0.9241 - val_loss: 0.2946 - val_precision: 0.9318 - val_recall: 0.9224\n",
      "Epoch 3/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 38ms/step - accuracy: 0.9836 - loss: 0.0495 - precision: 0.9852 - recall: 0.9820 - val_accuracy: 0.9270 - val_loss: 0.3015 - val_precision: 0.9308 - val_recall: 0.9258\n",
      "Epoch 4/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 38ms/step - accuracy: 0.9882 - loss: 0.0331 - precision: 0.9891 - recall: 0.9871 - val_accuracy: 0.9203 - val_loss: 0.3252 - val_precision: 0.9247 - val_recall: 0.9174\n",
      "Epoch 5/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 37ms/step - accuracy: 0.9894 - loss: 0.0318 - precision: 0.9904 - recall: 0.9886 - val_accuracy: 0.9388 - val_loss: 0.3271 - val_precision: 0.9435 - val_recall: 0.9379\n",
      "Epoch 6/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 37ms/step - accuracy: 0.9924 - loss: 0.0238 - precision: 0.9929 - recall: 0.9919 - val_accuracy: 0.9203 - val_loss: 0.3883 - val_precision: 0.9231 - val_recall: 0.9169\n",
      "Epoch 7/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 38ms/step - accuracy: 0.9935 - loss: 0.0221 - precision: 0.9939 - recall: 0.9932 - val_accuracy: 0.9400 - val_loss: 0.2766 - val_precision: 0.9427 - val_recall: 0.9388\n",
      "Epoch 8/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 38ms/step - accuracy: 0.9949 - loss: 0.0165 - precision: 0.9953 - recall: 0.9946 - val_accuracy: 0.9283 - val_loss: 0.3351 - val_precision: 0.9344 - val_recall: 0.9266\n",
      "Epoch 9/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 38ms/step - accuracy: 0.9942 - loss: 0.0200 - precision: 0.9944 - recall: 0.9939 - val_accuracy: 0.9320 - val_loss: 0.3407 - val_precision: 0.9353 - val_recall: 0.9279\n",
      "Epoch 10/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 38ms/step - accuracy: 0.9948 - loss: 0.0167 - precision: 0.9951 - recall: 0.9947 - val_accuracy: 0.9379 - val_loss: 0.2971 - val_precision: 0.9394 - val_recall: 0.9371\n",
      "Epoch 11/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 37ms/step - accuracy: 0.9963 - loss: 0.0119 - precision: 0.9964 - recall: 0.9961 - val_accuracy: 0.9291 - val_loss: 0.3427 - val_precision: 0.9329 - val_recall: 0.9279\n",
      "Epoch 12/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 38ms/step - accuracy: 0.9958 - loss: 0.0157 - precision: 0.9960 - recall: 0.9956 - val_accuracy: 0.9409 - val_loss: 0.3343 - val_precision: 0.9427 - val_recall: 0.9392\n",
      "Epoch 13/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 38ms/step - accuracy: 0.9966 - loss: 0.0106 - precision: 0.9967 - recall: 0.9964 - val_accuracy: 0.9115 - val_loss: 0.5696 - val_precision: 0.9140 - val_recall: 0.9098\n",
      "Epoch 14/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 38ms/step - accuracy: 0.9960 - loss: 0.0151 - precision: 0.9963 - recall: 0.9957 - val_accuracy: 0.9316 - val_loss: 0.3651 - val_precision: 0.9354 - val_recall: 0.9299\n",
      "Epoch 15/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 38ms/step - accuracy: 0.9973 - loss: 0.0097 - precision: 0.9974 - recall: 0.9972 - val_accuracy: 0.9358 - val_loss: 0.3661 - val_precision: 0.9381 - val_recall: 0.9341\n",
      "Epoch 16/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 38ms/step - accuracy: 0.9969 - loss: 0.0093 - precision: 0.9970 - recall: 0.9967 - val_accuracy: 0.9178 - val_loss: 0.4086 - val_precision: 0.9191 - val_recall: 0.9153\n",
      "Epoch 17/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 38ms/step - accuracy: 0.9970 - loss: 0.0092 - precision: 0.9971 - recall: 0.9968 - val_accuracy: 0.9136 - val_loss: 0.5256 - val_precision: 0.9166 - val_recall: 0.9123\n",
      "Final validation accuracy: 94.09%\n",
      "Saved model as  ResNetTrulyFineTuned_forShow94.09.keras\n"
     ]
    }
   ],
   "source": [
    "# ResNet training WITH FINE TUNING\n",
    "# Trained on the medium augmentation level, like the model that performed best on the test set\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = RN.fit(\n",
    "    x=X_train_RN,\n",
    "    y=y_train_RN,\n",
    "    batch_size=batch_size_RN,\n",
    "    epochs=epochs_RN,\n",
    "    validation_data=(X_val_RN, y_val_RN),\n",
    "    callbacks=callbacks_RN,\n",
    "    class_weight=class_weight_dict_RN\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "model_filename = 'ResNetTrulyFineTuned_forShow'+str(final_val_accuracy)+'.keras'\n",
    "RN.save(model_filename)\n",
    "print(\"Saved model as \", model_filename)\n",
    "\n",
    "# Delete the model to free up resources\n",
    "del RN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXiyNDv5Sf_-"
   },
   "source": [
    "## EfficientNetB4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GqssYvQ3Sv-W"
   },
   "outputs": [],
   "source": [
    "def preprocess_images_in_batches_efficientnet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_efficientnet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3-IYFtqVTQ9f"
   },
   "outputs": [],
   "source": [
    "# Scale input to EfficientNet-specific format\n",
    "X_train_EN = preprocess_images_in_batches_efficientnet(X_train)\n",
    "X_val_EN = preprocess_images_in_batches_efficientnet(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "OLscfEyvTTYJ"
   },
   "outputs": [],
   "source": [
    "# Make label tensors one-hot encoded\n",
    "y_train_EN = to_categorical(y_train)\n",
    "y_val_EN = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtE4whrWTUAi",
    "outputId": "53cf2248-6a5f-4751-a951-bc215f088277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (96, 96, 3)\n",
      "Output Shape: 8\n"
     ]
    }
   ],
   "source": [
    "# Input shape for the model\n",
    "input_shape_EN = X_train_EN.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape_EN = y_train_EN.shape[1]\n",
    "\n",
    "print(\"Input Shape:\", input_shape_EN)\n",
    "print(\"Output Shape:\", output_shape_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyp7OcvZUi-Y",
    "outputId": "7c4eeaba-b4f4-4a18-dc92-30db1499e612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1000\n",
      "Batch Size: 32\n",
      "Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "\n",
    "epochs_EN = 1000\n",
    "\n",
    "batch_size_EN = 32\n",
    "\n",
    "learning_rate_EN = 0.001\n",
    "\n",
    "print(\"Epochs:\", epochs_EN)\n",
    "print(\"Batch Size:\", batch_size_EN)\n",
    "print(\"Learning Rate:\", learning_rate_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4p5HgzGPT6J5",
    "outputId": "b4695dfd-d860-488f-f7e8-4c33c8e65f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 2.166440217391304, 1: 0.6357655502392344, 2: 1.519536213468869, 3: 0.6932608695652174, 4: 2.170372050816697, 5: 1.7231628242074928, 6: 0.5885211614173228, 7: 0.8891263940520446}\n"
     ]
    }
   ],
   "source": [
    "# Calculate appropriate class weights to make up for dataset imbalance\n",
    "\n",
    "classes = np.unique(np.argmax(y_train_EN, axis=1))\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=np.argmax(y_train_RN, axis=1))\n",
    "class_weight_dict_EN = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "dU8hQo4GTdvc"
   },
   "outputs": [],
   "source": [
    "# Implementation of EfficientNet, not fine-tunable (only used to select baseline model)\n",
    "\n",
    "def build_efficientnet(\n",
    "    input_shape=input_shape_EN,\n",
    "    output_shape=output_shape_EN,\n",
    "    learning_rate=learning_rate_EN,\n",
    "    seed=seed\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    base_model = EfficientNetB4(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='Input')\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=seed, name='dropout_1')(x)\n",
    "    outputs = tf.keras.layers.Dense(units=output_shape, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='EfficientNetB4')\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    mixed_precision_optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=mixed_precision_optimizer, metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "f1pRuw1LX8Yp",
    "outputId": "57aca484-e11c-4e88-b9b7-74ed672b8877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetB4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EfficientNetB4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ efficientnetb4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">229,504</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ efficientnetb4 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1792\u001b[0m)          │      \u001b[38;5;34m17,673,823\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m229,504\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,904,359</span> (68.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,904,359\u001b[0m (68.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">230,536</span> (900.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m230,536\u001b[0m (900.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> (67.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,673,823\u001b[0m (67.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EN = build_efficientnet()\n",
    "EN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "DkNXzSgYYNYW"
   },
   "outputs": [],
   "source": [
    "patience_EN = 5\n",
    "\n",
    "early_stopping_EN = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=patience_EN,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Store the callback in a list\n",
    "callbacks_EN = [early_stopping_EN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DN9rr-GJXyV0",
    "outputId": "1fc6201d-7f73-454a-c714-f9f579e9961f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 35ms/step - accuracy: 0.6433 - loss: 0.9772 - precision: 0.7791 - recall: 0.4970 - val_accuracy: 0.8171 - val_loss: 0.5261 - val_precision: 0.8681 - val_recall: 0.7567\n",
      "Epoch 2/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 18ms/step - accuracy: 0.7748 - loss: 0.6108 - precision: 0.8342 - recall: 0.7100 - val_accuracy: 0.8318 - val_loss: 0.4822 - val_precision: 0.8728 - val_recall: 0.7940\n",
      "Epoch 3/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.7973 - loss: 0.5406 - precision: 0.8476 - recall: 0.7465 - val_accuracy: 0.8368 - val_loss: 0.4707 - val_precision: 0.8735 - val_recall: 0.8108\n",
      "Epoch 4/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 18ms/step - accuracy: 0.8087 - loss: 0.5094 - precision: 0.8534 - recall: 0.7620 - val_accuracy: 0.8473 - val_loss: 0.4330 - val_precision: 0.8813 - val_recall: 0.8188\n",
      "Epoch 5/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 18ms/step - accuracy: 0.8161 - loss: 0.4827 - precision: 0.8568 - recall: 0.7744 - val_accuracy: 0.8486 - val_loss: 0.4317 - val_precision: 0.8742 - val_recall: 0.8305\n",
      "Epoch 6/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 18ms/step - accuracy: 0.8250 - loss: 0.4634 - precision: 0.8631 - recall: 0.7858 - val_accuracy: 0.8507 - val_loss: 0.4569 - val_precision: 0.8791 - val_recall: 0.8238\n",
      "Epoch 7/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8308 - loss: 0.4368 - precision: 0.8676 - recall: 0.7944 - val_accuracy: 0.8637 - val_loss: 0.4197 - val_precision: 0.8924 - val_recall: 0.8352\n",
      "Epoch 8/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8353 - loss: 0.4234 - precision: 0.8701 - recall: 0.8023 - val_accuracy: 0.8620 - val_loss: 0.4104 - val_precision: 0.8834 - val_recall: 0.8423\n",
      "Epoch 9/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8362 - loss: 0.4146 - precision: 0.8738 - recall: 0.8040 - val_accuracy: 0.8624 - val_loss: 0.4317 - val_precision: 0.8794 - val_recall: 0.8469\n",
      "Epoch 10/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8424 - loss: 0.4087 - precision: 0.8733 - recall: 0.8093 - val_accuracy: 0.8616 - val_loss: 0.4088 - val_precision: 0.8838 - val_recall: 0.8452\n",
      "Epoch 11/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.8452 - loss: 0.4002 - precision: 0.8774 - recall: 0.8146 - val_accuracy: 0.8633 - val_loss: 0.4200 - val_precision: 0.8812 - val_recall: 0.8465\n",
      "Epoch 12/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.8460 - loss: 0.3939 - precision: 0.8783 - recall: 0.8156 - val_accuracy: 0.8691 - val_loss: 0.4124 - val_precision: 0.8842 - val_recall: 0.8549\n",
      "Epoch 13/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.8521 - loss: 0.3803 - precision: 0.8813 - recall: 0.8225 - val_accuracy: 0.8687 - val_loss: 0.3997 - val_precision: 0.8939 - val_recall: 0.8377\n",
      "Epoch 14/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8544 - loss: 0.3735 - precision: 0.8832 - recall: 0.8258 - val_accuracy: 0.8658 - val_loss: 0.4189 - val_precision: 0.8879 - val_recall: 0.8536\n",
      "Epoch 15/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8535 - loss: 0.3700 - precision: 0.8820 - recall: 0.8259 - val_accuracy: 0.8637 - val_loss: 0.4082 - val_precision: 0.8895 - val_recall: 0.8440\n",
      "Epoch 16/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8598 - loss: 0.3632 - precision: 0.8870 - recall: 0.8305 - val_accuracy: 0.8695 - val_loss: 0.3908 - val_precision: 0.8884 - val_recall: 0.8544\n",
      "Epoch 17/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8583 - loss: 0.3545 - precision: 0.8855 - recall: 0.8338 - val_accuracy: 0.8666 - val_loss: 0.4038 - val_precision: 0.8809 - val_recall: 0.8498\n",
      "Epoch 18/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8627 - loss: 0.3493 - precision: 0.8862 - recall: 0.8378 - val_accuracy: 0.8670 - val_loss: 0.3795 - val_precision: 0.8896 - val_recall: 0.8549\n",
      "Epoch 19/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8631 - loss: 0.3424 - precision: 0.8882 - recall: 0.8373 - val_accuracy: 0.8746 - val_loss: 0.3929 - val_precision: 0.8862 - val_recall: 0.8591\n",
      "Epoch 20/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8676 - loss: 0.3368 - precision: 0.8926 - recall: 0.8433 - val_accuracy: 0.8817 - val_loss: 0.3796 - val_precision: 0.8933 - val_recall: 0.8708\n",
      "Epoch 21/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8684 - loss: 0.3397 - precision: 0.8932 - recall: 0.8468 - val_accuracy: 0.8742 - val_loss: 0.3890 - val_precision: 0.8894 - val_recall: 0.8637\n",
      "Epoch 22/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.8703 - loss: 0.3308 - precision: 0.8930 - recall: 0.8471 - val_accuracy: 0.8704 - val_loss: 0.3969 - val_precision: 0.8874 - val_recall: 0.8599\n",
      "Epoch 23/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.8706 - loss: 0.3296 - precision: 0.8933 - recall: 0.8482 - val_accuracy: 0.8763 - val_loss: 0.3838 - val_precision: 0.8946 - val_recall: 0.8654\n",
      "Epoch 24/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 18ms/step - accuracy: 0.8701 - loss: 0.3232 - precision: 0.8928 - recall: 0.8471 - val_accuracy: 0.8641 - val_loss: 0.4083 - val_precision: 0.8775 - val_recall: 0.8507\n",
      "Epoch 25/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.8712 - loss: 0.3263 - precision: 0.8941 - recall: 0.8487 - val_accuracy: 0.8725 - val_loss: 0.3964 - val_precision: 0.8909 - val_recall: 0.8595\n",
      "Final validation accuracy: 88.17%\n",
      "Saved model as  EffNet_NonFineTunedForShow88.17.keras\n"
     ]
    }
   ],
   "source": [
    "# EfficientNet training\n",
    "# Trained on the baseline level of augmentation, mimicking the test we did to choose our baseline model\n",
    "\n",
    "# Train the model with early stopping callback\n",
    "history = EN.fit(\n",
    "    x=X_train_EN,\n",
    "    y=y_train_EN,\n",
    "    batch_size=batch_size_EN,\n",
    "    epochs=epochs_EN,\n",
    "    validation_data=(X_val_EN, y_val_EN),\n",
    "    callbacks=callbacks_EN,\n",
    "    class_weight=class_weight_dict_EN\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "model_filename = 'EffNet_NonFineTunedForShow'+str(final_val_accuracy)+'.keras'\n",
    "EN.save(model_filename)\n",
    "print(\"Saved model as \", model_filename)\n",
    "\n",
    "# Delete the model to free up resources\n",
    "del EN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K2oj0biYjg9"
   },
   "source": [
    "## MobileNetV3Small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tZ0TsQyQYjFK"
   },
   "outputs": [],
   "source": [
    "def preprocess_images_mobilenet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_mobilenet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch (not strictly necessary in Python)\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BUg7ABAUa8Q3"
   },
   "outputs": [],
   "source": [
    "# Scale input to MobileNet-specific format\n",
    "X_train_MN = preprocess_images_mobilenet(X_train)\n",
    "X_val_MN = preprocess_images_mobilenet(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gs5t7O2CbCU3"
   },
   "outputs": [],
   "source": [
    "# Make label tensors one-hot encoded\n",
    "y_train_MN = to_categorical(y_train)\n",
    "y_val_MN = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnAs2kUMbDlo",
    "outputId": "7843376a-a663-422d-e28b-554c213d7247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (96, 96, 3)\n",
      "Output Shape: 8\n"
     ]
    }
   ],
   "source": [
    "# Input shape for the model\n",
    "input_shape_MN = X_train_MN.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape_MN = y_train_MN.shape[1]\n",
    "\n",
    "print(\"Input Shape:\", input_shape_MN)\n",
    "print(\"Output Shape:\", output_shape_MN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC4zPDBHbIg7",
    "outputId": "c220c259-b5b4-4a03-aca7-fa5d7aa19a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1000\n",
      "Batch Size: 32\n",
      "Learning Rare: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "epochs_MN = 1000\n",
    "\n",
    "batch_size_MN = 32\n",
    "\n",
    "learning_rate_MN = 0.001\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs_MN)\n",
    "print(\"Batch Size:\", batch_size_MN)\n",
    "print(\"Learning Rare:\", learning_rate_MN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_Y9-ik3bORq",
    "outputId": "04c4e32d-8bcb-45c1-9305-8aca5acc76f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 2.166440217391304, 1: 0.6357655502392344, 2: 1.519536213468869, 3: 0.6932608695652174, 4: 2.170372050816697, 5: 1.7231628242074928, 6: 0.5885211614173228, 7: 0.8891263940520446}\n"
     ]
    }
   ],
   "source": [
    "# Calculate appropriate class weights to make up for dataset imbalance\n",
    "classes = np.unique(np.argmax(y_train_MN, axis=1))\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=np.argmax(y_train_MN, axis=1))\n",
    "class_weight_dict_MN = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weight_dict_MN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "vDMImMxQb7lY"
   },
   "outputs": [],
   "source": [
    "# Implementation of MobileNetV3, not fine-tunable (only used to select baseline model)\n",
    "\n",
    "def build_mobilenet_v3_small(\n",
    "    input_shape=input_shape_MN,\n",
    "    output_shape=output_shape_MN,\n",
    "    learning_rate=learning_rate_MN,\n",
    "    seed=seed\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Enable mixed precision\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision policy set to: 'mixed_float16'\")\n",
    "\n",
    "    # Load MobileNetV3Small as the base model\n",
    "    base_model = MobileNetV3Small(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom classification head\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, name='Input')\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, seed=seed, name='dropout_1')(x)\n",
    "    outputs = tf.keras.layers.Dense(units=output_shape, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='MobileNetV3Small')\n",
    "\n",
    "    # Compile the model with mixed precision optimizer\n",
    "    adam_optimizer = Adam(learning_rate=learning_rate)\n",
    "    mixed_precision_optimizer = LossScaleOptimizer(adam_optimizer)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=mixed_precision_optimizer,\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "XFyjsOcdc6UI",
    "outputId": "74e5f251-78e2-49a6-8c8a-ccf9d456991a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision policy set to: 'mixed_float16'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/applications/mobilenet_v3.py:452: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  return MobileNetV3(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"MobileNetV3Small\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"MobileNetV3Small\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ MobileNetV3Small (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ MobileNetV3Small (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m576\u001b[0m)           │         \u001b[38;5;34m939,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_avg_pool                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m1,032\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,014,008</span> (3.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,014,008\u001b[0m (3.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,888</span> (292.53 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m74,888\u001b[0m (292.53 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MN = build_mobilenet_v3_small()\n",
    "MN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ljuMFrjAdIWT"
   },
   "outputs": [],
   "source": [
    "patience_MN = 5\n",
    "\n",
    "early_stopping_MN = tfk.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=patience_MN,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks_MN = [early_stopping_MN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYnWO0SCdYQO",
    "outputId": "f177e07e-6811-4470-e6d5-4716fe3e82c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 14ms/step - accuracy: 0.3191 - loss: 1.6861 - precision: 0.7745 - recall: 0.0549 - val_accuracy: 0.5101 - val_loss: 1.3393 - val_precision: 0.8463 - val_recall: 0.1732\n",
      "Epoch 2/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.4650 - loss: 1.3454 - precision: 0.7943 - recall: 0.2117 - val_accuracy: 0.5508 - val_loss: 1.2397 - val_precision: 0.8073 - val_recall: 0.2601\n",
      "Epoch 3/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.5132 - loss: 1.2530 - precision: 0.7757 - recall: 0.2747 - val_accuracy: 0.5831 - val_loss: 1.1643 - val_precision: 0.8709 - val_recall: 0.2999\n",
      "Epoch 4/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.5434 - loss: 1.1898 - precision: 0.7737 - recall: 0.3165 - val_accuracy: 0.6170 - val_loss: 1.0746 - val_precision: 0.8444 - val_recall: 0.3687\n",
      "Epoch 5/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.5670 - loss: 1.1370 - precision: 0.7743 - recall: 0.3557 - val_accuracy: 0.6263 - val_loss: 1.0313 - val_precision: 0.8424 - val_recall: 0.4081\n",
      "Epoch 6/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.5802 - loss: 1.0946 - precision: 0.7776 - recall: 0.3843 - val_accuracy: 0.6384 - val_loss: 0.9866 - val_precision: 0.8310 - val_recall: 0.4497\n",
      "Epoch 7/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.5961 - loss: 1.0646 - precision: 0.7764 - recall: 0.4097 - val_accuracy: 0.6393 - val_loss: 0.9693 - val_precision: 0.8400 - val_recall: 0.4471\n",
      "Epoch 8/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6001 - loss: 1.0436 - precision: 0.7806 - recall: 0.4227 - val_accuracy: 0.6405 - val_loss: 0.9770 - val_precision: 0.8260 - val_recall: 0.4639\n",
      "Epoch 9/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6012 - loss: 1.0324 - precision: 0.7748 - recall: 0.4267 - val_accuracy: 0.6414 - val_loss: 0.9387 - val_precision: 0.8218 - val_recall: 0.4702\n",
      "Epoch 10/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6139 - loss: 1.0071 - precision: 0.7802 - recall: 0.4442 - val_accuracy: 0.6611 - val_loss: 0.9170 - val_precision: 0.8331 - val_recall: 0.4899\n",
      "Epoch 11/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6182 - loss: 1.0029 - precision: 0.7833 - recall: 0.4499 - val_accuracy: 0.6674 - val_loss: 0.9018 - val_precision: 0.8238 - val_recall: 0.4962\n",
      "Epoch 12/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6128 - loss: 1.0040 - precision: 0.7762 - recall: 0.4441 - val_accuracy: 0.6565 - val_loss: 0.9231 - val_precision: 0.8370 - val_recall: 0.4954\n",
      "Epoch 13/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6178 - loss: 0.9941 - precision: 0.7813 - recall: 0.4488 - val_accuracy: 0.6636 - val_loss: 0.9157 - val_precision: 0.8414 - val_recall: 0.4962\n",
      "Epoch 14/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6231 - loss: 0.9814 - precision: 0.7811 - recall: 0.4589 - val_accuracy: 0.6711 - val_loss: 0.9001 - val_precision: 0.8177 - val_recall: 0.5231\n",
      "Epoch 15/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6268 - loss: 0.9688 - precision: 0.7860 - recall: 0.4689 - val_accuracy: 0.6678 - val_loss: 0.8922 - val_precision: 0.8362 - val_recall: 0.5034\n",
      "Epoch 16/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6309 - loss: 0.9604 - precision: 0.7862 - recall: 0.4717 - val_accuracy: 0.6837 - val_loss: 0.8673 - val_precision: 0.8374 - val_recall: 0.5294\n",
      "Epoch 17/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6362 - loss: 0.9427 - precision: 0.7875 - recall: 0.4823 - val_accuracy: 0.6766 - val_loss: 0.8697 - val_precision: 0.8373 - val_recall: 0.5268\n",
      "Epoch 18/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6331 - loss: 0.9460 - precision: 0.7835 - recall: 0.4805 - val_accuracy: 0.6741 - val_loss: 0.8679 - val_precision: 0.8339 - val_recall: 0.5264\n",
      "Epoch 19/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6352 - loss: 0.9393 - precision: 0.7804 - recall: 0.4870 - val_accuracy: 0.6841 - val_loss: 0.8519 - val_precision: 0.8246 - val_recall: 0.5482\n",
      "Epoch 20/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6379 - loss: 0.9363 - precision: 0.7864 - recall: 0.4887 - val_accuracy: 0.6791 - val_loss: 0.8615 - val_precision: 0.8233 - val_recall: 0.5336\n",
      "Epoch 21/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6400 - loss: 0.9345 - precision: 0.7851 - recall: 0.4927 - val_accuracy: 0.6846 - val_loss: 0.8408 - val_precision: 0.8155 - val_recall: 0.5524\n",
      "Epoch 22/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6406 - loss: 0.9262 - precision: 0.7859 - recall: 0.4964 - val_accuracy: 0.6841 - val_loss: 0.8587 - val_precision: 0.8445 - val_recall: 0.5378\n",
      "Epoch 23/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6414 - loss: 0.9244 - precision: 0.7891 - recall: 0.4979 - val_accuracy: 0.6804 - val_loss: 0.8608 - val_precision: 0.8007 - val_recall: 0.5579\n",
      "Epoch 24/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6450 - loss: 0.9148 - precision: 0.7843 - recall: 0.5024 - val_accuracy: 0.6837 - val_loss: 0.8414 - val_precision: 0.8328 - val_recall: 0.5516\n",
      "Epoch 25/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6480 - loss: 0.9063 - precision: 0.7864 - recall: 0.5092 - val_accuracy: 0.6871 - val_loss: 0.8415 - val_precision: 0.8371 - val_recall: 0.5541\n",
      "Epoch 26/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6512 - loss: 0.9014 - precision: 0.7889 - recall: 0.5129 - val_accuracy: 0.6980 - val_loss: 0.8188 - val_precision: 0.8269 - val_recall: 0.5730\n",
      "Epoch 27/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6534 - loss: 0.8978 - precision: 0.7902 - recall: 0.5134 - val_accuracy: 0.6904 - val_loss: 0.8384 - val_precision: 0.8301 - val_recall: 0.5575\n",
      "Epoch 28/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6527 - loss: 0.9010 - precision: 0.7923 - recall: 0.5132 - val_accuracy: 0.6871 - val_loss: 0.8423 - val_precision: 0.8144 - val_recall: 0.5612\n",
      "Epoch 29/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6543 - loss: 0.8925 - precision: 0.7908 - recall: 0.5188 - val_accuracy: 0.6753 - val_loss: 0.8603 - val_precision: 0.8017 - val_recall: 0.5562\n",
      "Epoch 30/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.6519 - loss: 0.8958 - precision: 0.7906 - recall: 0.5175 - val_accuracy: 0.6808 - val_loss: 0.8241 - val_precision: 0.8144 - val_recall: 0.5759\n",
      "Epoch 31/1000\n",
      "\u001b[1m2392/2392\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.6567 - loss: 0.8868 - precision: 0.7923 - recall: 0.5228 - val_accuracy: 0.6787 - val_loss: 0.8443 - val_precision: 0.8362 - val_recall: 0.5310\n",
      "Final validation accuracy: 69.8%\n",
      "Saved model as  MobileNet_NotFineTuned69.8.keras\n"
     ]
    }
   ],
   "source": [
    "#MobileNet training\n",
    "# Trained on the baseline level of augmentation, mimicking the test we did to choose our baseline model\n",
    "\n",
    "history = MN.fit(\n",
    "    x=X_train_MN,\n",
    "    y=y_train_MN,\n",
    "    batch_size=batch_size_MN,\n",
    "    epochs=epochs_MN,\n",
    "    validation_data=(X_val_MN, y_val_MN),\n",
    "    callbacks=callbacks_MN,\n",
    "    class_weight=class_weight_dict_MN\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
    "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
    "\n",
    "model_filename = 'MobileNet_NotFineTuned'+str(final_val_accuracy)+'.keras'\n",
    "MN.save(model_filename)\n",
    "print(\"Saved model as \", model_filename)\n",
    "\n",
    "# Delete the model to free up resources\n",
    "del MN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj3Gk1SVdlS2"
   },
   "source": [
    "# Preparing for hand-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoLuI3KXdn6H"
   },
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_resnet\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_efficientnet\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input as preprocess_mobilenet\n",
    "\n",
    "\n",
    "def preprocess_images_in_batches_efficientnet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_efficientnet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images\n",
    "\n",
    "def preprocess_images_in_batches_resnet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_resnet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch (not strictly necessary in Python)\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images\n",
    "\n",
    "def preprocess_images_mobilenet(images, batch_size=1024):\n",
    "    # Placeholder for the preprocessed dataset\n",
    "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min(start + batch_size, len(images))\n",
    "\n",
    "        # Preprocess the current batch\n",
    "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
    "        preprocessed_images[start:end] = preprocess_mobilenet(batch)\n",
    "\n",
    "        # Free up memory by deleting the batch (not strictly necessary in Python)\n",
    "        del batch\n",
    "\n",
    "    return preprocessed_images\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.neural_network = tfk.models.load_model('YOU FORGOT THE FILENAME')\n",
    "        self.neural_network.trainable = False\n",
    "\n",
    "    def predict(self, X):\n",
    "    #NB! Choose correct normalization function depending on model being evaluated!\n",
    "        X_scaled = preprocess_images_in_batches_resnet(X)\n",
    "        preds = self.neural_network.predict(X_scaled)\n",
    "\n",
    "        if len(preds.shape) == 2:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7U-mRfF2LQR"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
    "\n",
    "# Add files to the zip command if needed\n",
    "!zip {filename} model.py MODEL FILENAME\n",
    "\n",
    "from google.colab import files\n",
    "files.download(filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
